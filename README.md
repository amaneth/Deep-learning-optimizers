# Optimizers-implementation-from-scratch
We implement different deep learning optimizers from scratch. They are Batch Gradient Descent, Stochastic Gradient Descent, Mini-batch Gradient Descent, Momentum, Adadelta, Adagrad, RMSprop, and Adam.
